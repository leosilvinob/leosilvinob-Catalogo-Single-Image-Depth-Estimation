{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Catálogo Single Image Depth Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbX//lVGoA+1qWcsfq8VD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leosilvinob/leosilvinob-Catalogo-Single-Image-Depth-Estimation/blob/main/Cat%C3%A1logo_Single_Image_Depth_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Catálogo de modelos para Profundidade de Imagens\n",
        "Single image depth estimation também chamado de SIDE é a tarefa de estimar uma densidade mapa de profundidade para uma determinada imagem RGB. Mais especificamente, para cada pixel na imagem RGB dada, é preciso estimar uma métrica valor de profundidade.\n",
        "\n",
        "Devido às suas propriedades, o problema de estimativa de profundidade de imagem única é atualmente melhor abordado com métodos de aprendizado de máquina, com mais sucesso com redes neurais convolucionais.\n",
        "\n",
        "Este catálogo tem como reumir diversos SIDEs, com suas introduções, links para código e artigo, assim como suas diferenças em relação a outros, sempre que possível, cada método tem sua data de publicação entre parênteses, isso não quer dizer que eles não sofram atualizações/melhorias.\n",
        "\n",
        "Todos estes métodos apresentados tem utilizam o dataset NYU-Depth V2 (32 GB de imagens) pelo menos como um de seus benchmarkings. Não será como foco principal o ranking de cada método no benchmarking, mas sim o diferencial que cada método pode trazer."
      ],
      "metadata": {
        "id": "yyrTSFpSvT8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fast Depth: Fast Monocular Depth Estimation on Embedded Systems (2019)\n",
        "Fast Depth é um projeto da MIT, que tem como objetivor realizar um modelo SIDE, esse projeto ja teve varias versões, também acompanha um dataset de imagens que podem ser usadas para treino e estimação de profundidade. O código foi desensvolvido em um sistema usando  PyTorch v0.4.1 e usa de redes neurais convolucionais(CNN). O grande diferencial deste modelo é sua eficiência para smartphones e sistemas embarcados.\n",
        "Para maiores informações, como usar o código, artigo cientifico e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/dwofk/fast-depth"
      ],
      "metadata": {
        "id": "QDExxSfcxG_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network(2018)\n",
        "\n",
        "Modelo criado por pesquisadores da Baidu, na china. O metódo tem como proposta o uso de uma rede convolucional simples, mas eficaz rede de propagação espacial (CSPN) para aprender a matriz de afinidade para previsão de profundidade. Especificamente, é adotado um modelo de propagação linear eficiente, onde a propagação é realizada com uma forma de operação convolucional recorrente, e a\n",
        "a afinidade entre os pixels vizinhos é aprendida através de uma rede neural convolucional profunda (CNN). \n",
        "\n",
        "É aplicado o CSPN projetado a duas tarefas de estimativa de profundidade dada uma única imagem: (1) Refinar a saída de profundidade do estado da arte existente métodos (SOTA); (2) Convertendo as amostras de profundidade esparsas em um mapa de profundidade densa incorporando as amostras de profundidade dentro do procedimento de propagação. A segunda tarefa é inspirado pela disponibilidade do LiDAR(Light Detection and Ranging) que fornece medições de profundidade esparsas, mas precisas.\n",
        "\n",
        "Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/XinJCheng/CSPN\n"
      ],
      "metadata": {
        "id": "EUOgx21lBoqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deep Ordinal Regression Network for Monocular Depth Estimation (2018)\n",
        "  Também chamado DORN, tem o conceito de introduzir uma estratégia de discretização de aumento de espaçamento (SID) para discretizar a profundidade e reformular o aprendizado de rede de profundidade como um problema de regressão ordinal. Ao treinar a rede usando uma perda de regressão comum, nosso método atinge uma precisão muito maior e uma convergência mais rápida em sincronia. Além disso, adotamos uma estrutura de rede multiescala que evita agrupamentos espaciais desnecessários e captura informações multiescala em paralelo.\n",
        "  Vale notar que este modelo tem performance melhor que **Fast Depth**, entretanto ele tem mais parametros que o Fast Depth, o que faz com que sua perfomance em tempo-real em sistemas embarcados seja inferior. O DORN foi vencedor da Robust Vision Challange 2018.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/hufu6371/DORN"
      ],
      "metadata": {
        "id": "HrVR452HEpck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image (2017)\n",
        "\n",
        "O modelo é foi realizado considerando o problema da previsão de profundidade densa a partir de um conjunto esparso de medidas de profundidade e uma única imagem RGB. \n",
        "\n",
        "Como a estimativa de profundidade apenas a partir de imagens monoculares é inerentemente ambígua e não confiável, para atingir um nível mais alto de robustez e precisão, é introduzido amostras de profundidade esparsas adicionais, que são adquiridas com um sensor de profundidade de baixa resolução ou computadas via localização e mapeamento simultâneos visuais (SLAM). \n",
        "\n",
        "É proposto o uso de uma única rede de regressão profunda para aprender diretamente com os dados brutos RGB-D e explorar o impacto do número de amostras de profundidade na precisão da previsão\n",
        ".\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/fangchangma/sparse-to-dense"
      ],
      "metadata": {
        "id": "wbVSrdpxJPGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation(2018)\n",
        "\n",
        "O modelo consiste no uso do método que emprega um CRF(Conditional Random Fields) contínuo para fundir informações multi-escala derivadas de diferentes camadas de uma Rede Neural Convolucional (CNN) de front-end. Utilizando um modelo de atenção estruturado que regula automaticamente a quantidade de informações transferidas entre os recursos correspondentes em diferentes escalas. É importante ressaltar que o modelo de atenção proposto é perfeitamente integrado ao CRF, permitindo o treinamento de ponta a ponta de toda a arquitetura.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/danxuhk/StructuredAttentionDepthEstimation\n"
      ],
      "metadata": {
        "id": "My0x0mh2KXrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction(2021)\n",
        "\n",
        "Este modelo é projetado um termo de perda que impõe uma restrição geométrica simples, ou seja, direções normais virtuais determinadas por três pontos amostrados aleatoriamente no espaço 3D reconstruído, melhorando significativamente a precisão e a robustez da estimativa de profundidade monocular. Significativamente, a perda normal virtual pode não apenas melhorar o desempenho do aprendizado da profundidade métrica, mas também separar as informações de escala e enriquecer o modelo com melhores informações de forma. Portanto, quando não se têm acesso a dados de treinamento de profundidade métrica absoluta, pode-se usar o virtual normal para aprender uma profundidade afim invariante robusta gerada em diversas cenas.\n",
        "\n",
        "O maior diferencial deste método é que ele é capaz de lidar com imagens mais complexas, aumentando a eficiência quando se trata de recuperar a geometria 3D da forma da cena.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/YvanYin/DiverseDepth\n"
      ],
      "metadata": {
        "id": "wLp8b5xOMvEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention-based Context Aggregation Network for Monocular Depth Estimation(2018)\n",
        "\n",
        "Neste método é proposto uma rede de agregação de contexto baseada na atenção (ACAN) para enfrentar essas dificuldades. Com base no modelo de autoatenção, o ACAN aprende de forma adaptativa as semelhanças específicas da tarefa entre os pixels para modelar as informações de contexto. Primeiro, é reformulado a estimativa de profundidade monocular como um problema de classificação multiclasse de rotulagem densa. Em seguida, proposto uma inferência ordinal suave para transformar as probabilidades previstas para valores de profundidade contínua, o que pode reduzir o erro de discretização (redução de cerca de 1% no RMSE). \n",
        "\n",
        "Em segundo lugar, o ACAN proposto agrega as informações de contexto em nível de imagem e em nível de pixel para estimativa de profundidade, onde o primeiro expressa a característica estatística de toda a imagem e o último extrai as dependências espaciais de longo alcance para cada pixel. Terceiro, para reduzir ainda mais a inconsistência entre a imagem RGB e o mapa de profundidade, construímos uma perda de atenção para minimizar sua entropia de informação.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/miraiaroha/ACAN"
      ],
      "metadata": {
        "id": "eCqtu4C5QAKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating (2021)\n",
        "Este método utiliza Recurrent Multi-Scale Feature Modulation (R-MSFM), uma nova arquitetura de rede profunda para estimativa de profundidade monocular auto-supervisionada. O R-MSFM extrai recursos por pixel, cria um módulo de modulação de recursos em várias escalas e atualiza iterativamente uma profundidade inversa por meio de um decodificador de parâmetro compartilhado na resolução fixa. O diferencial deste método é sua eficiência lidando com resoluções mais altas como de 1024 × 320. Além de também ter alta eficiência lidando com imagens de interior, mesmo que complexas.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/jsczzzk/R-MSFM"
      ],
      "metadata": {
        "id": "3kDDoTy1ThOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis (2021)\n",
        "Este modelo tem como proposta o uso do MINE para realizar novas sínteses de vistas e estimativas de profundidade por meio de reconstrução 3D densa a partir de uma única imagem. A abordagem é uma generalização de profundidade contínua das **I**magens **M**ultiplanares (MPI) introduzindo os campos de radiância **NE**ural (NeRF). \n",
        "\n",
        "Dada uma única imagem como entrada, o MINE prevê uma imagem de 4 canais (RGB e densidade de volume) em valores de profundidade arbitrários para reconstruir conjuntamente o tronco da câmera e preencher o conteúdo ocluído. O tronco reconstruído e pintado pode ser facilmente renderizado em novas visualizações RGB ou de profundidade usando renderização diferenciável.\n",
        "\n",
        "Este modelo é bastante eficaz para síntese de visualização, que é o seu diferencial, ao mesmo tempo que fornece uma estimação de profundidade de imagens únicas com ótima eficiência.\n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "\n",
        "https://github.com/jsczzzk/R-MSFM"
      ],
      "metadata": {
        "id": "YhJgdAoHZTx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3D Ken Burns Effect from a Single Image (2019)\n",
        "Este método tem uma estrutura que primeiro aproveita um pipeline de previsão de profundidade, que estima a profundidade da cena adequada para tarefas de síntese de visualização. Para abordar as limitações dos métodos de estimativa de profundidade existentes, como distorções geométricas, distorções semânticas e limites de profundidade imprecisos, foi desenvolvido uma rede neural semântica para previsão de profundidade, acoplamos sua estimativa a um processo de ajuste de profundidade baseado em segmentação e empregamos um refinamento rede neural que facilita previsões de profundidade precisas nos limites do objeto. \n",
        "\n",
        "De acordo com essa estimativa de profundidade, a estrutura mapeia a imagem de entrada para uma nuvem de pontos e sintetiza os quadros de vídeo resultantes renderizando a nuvem de pontos a partir das posições de câmera correspondentes. \n",
        "\n",
        "Este método é o melhor ranqueado(1º lugar) no benchmarking: on Depth Estimation on NYU-Depth V2. \n",
        "\n",
        "  Para maiores informações, como usar o código, artigos cientificos e o código, tudo isso pode ser encontrado em:\n",
        "  \n",
        "https://github.com/sniklaus/3d-ken-burns"
      ],
      "metadata": {
        "id": "F0dFtLFUd69c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2SlFO36Kfpm-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}